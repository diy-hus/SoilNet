{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffe4d9-53e0-48ad-b3ab-365e1ba12eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.6.0+cu118\n",
      "Total labeled images before check: 2057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12b55bb52544b768aa86b7d62fe610c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image error: F:\\Soil_Labeled_Data\\M_10_20\\L_60_70\\alluvial\\IMG_0683.JPG\n",
      "Replaced with: F:\\Soil_Labeled_Data\\augmented_fallback\\aug_IMG_0683.JPG\n",
      "Image error: F:\\Soil_Labeled_Data\\M_10_20\\L_70_80\\alluvial\\IMG_0703.JPG\n",
      "Replaced with: F:\\Soil_Labeled_Data\\augmented_fallback\\aug_IMG_0703.JPG\n",
      "Image error: F:\\Soil_Labeled_Data\\M_10_20\\L_80_90\\alluvial\\IMG_0746.JPG\n",
      "Replaced with: F:\\Soil_Labeled_Data\\augmented_fallback\\aug_IMG_0746.JPG\n",
      "Image error: F:\\Soil_Labeled_Data\\M_10_20\\L_80_90\\alluvial\\IMG_0753.JPG\n",
      "Replaced with: F:\\Soil_Labeled_Data\\augmented_fallback\\aug_IMG_0753.JPG\n",
      "Image error: F:\\Soil_Labeled_Data\\M_10_20\\L_80_90\\alluvial\\IMG_0755.JPG\n",
      "Replaced with: F:\\Soil_Labeled_Data\\augmented_fallback\\aug_IMG_0755.JPG\n",
      "Replaced 5 invalid images with augmented versions.\n",
      "Loaded pretrained weights from C:\\Users\\PC\\soilNet\\checkpoints_VicReg_tinyImagenet\\vicreg_model_final_mu_30.0.pth\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f64209470f74b428e4a208793984f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss â†’ Total: 2.2389, Regression: 0.0868, Classification: 2.1521\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import timm\n",
    "import os\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "df = pd.read_csv(r\"F:\\Soil_Labeled_Data\\labels.csv\")\n",
    "print(f\"Total labeled images before check: {len(df)}\")\n",
    "\n",
    "required_columns = ['path', 'SM_0', 'SM_20', 'light_value', 'moisture_class']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"DataFrame missing columns: {missing_columns}\")\n",
    "\n",
    "fallback_dir = r\"F:\\Soil_Labeled_Data\\augmented_fallback\"\n",
    "os.makedirs(fallback_dir, exist_ok=True)\n",
    "replaced_count = 0\n",
    "augment = transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    path = row[\"path\"]\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img.verify()\n",
    "    except:\n",
    "        print(f\"Image error: {path}\")\n",
    "        folder = os.path.dirname(path)\n",
    "        all_images = [f for f in os.listdir(folder) if f.lower().endswith((\".jpg\", \".png\"))]\n",
    "        good_images = [f for f in all_images if f != os.path.basename(path)]\n",
    "        if not good_images:\n",
    "            print(f\"No replacement images in: {folder}\")\n",
    "            continue\n",
    "        candidate = random.choice(good_images)\n",
    "        candidate_path = os.path.join(folder, candidate)\n",
    "        try:\n",
    "            img = Image.open(candidate_path).convert(\"RGB\")\n",
    "            img_aug = augment(img)\n",
    "            new_filename = f\"aug_{os.path.basename(path)}\"\n",
    "            new_path = os.path.join(fallback_dir, new_filename)\n",
    "            img_aug.save(new_path)\n",
    "            df.at[idx, \"path\"] = new_path\n",
    "            replaced_count += 1\n",
    "            print(f\"Replaced with: {new_path}\")\n",
    "        except:\n",
    "            print(f\"Replacement image error: {candidate_path}\")\n",
    "            continue\n",
    "print(f\"Replaced {replaced_count} invalid images with augmented versions.\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class SoilDualTaskDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        light = torch.tensor([row[\"light_value\"] / 100.0], dtype=torch.float32)\n",
    "        regression_target = torch.tensor([row[\"SM_0\"] / 100.0, row[\"SM_20\"] / 100.0], dtype=torch.float32)\n",
    "        class_target = torch.tensor(row[\"moisture_class\"], dtype=torch.long)\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"light\": light,\n",
    "            \"regression_target\": regression_target,\n",
    "            \"class_target\": class_target\n",
    "        }\n",
    "\n",
    "class SoilNetDualHead(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.mnv2_block1 = nn.Sequential(*list(\n",
    "            timm.create_model(\"mobilenetv2_100.ra_in1k\", pretrained=True).blocks.children())[0:3]\n",
    "        )\n",
    "        self.channel_adapter = nn.Conv2d(32, 16, kernel_size=1, bias=False)\n",
    "        self.mobilevit_full = timm.create_model(\"mobilevitv2_050\", pretrained=True)\n",
    "        self.mobilevit_encoder = self.mobilevit_full.stages\n",
    "        self.mvit_to_mnv2 = nn.Conv2d(256, 32, kernel_size=1, bias=False)\n",
    "        self.mnv2_block2 = nn.Sequential(*list(\n",
    "            timm.create_model(\"mobilenetv2_100.ra_in1k\", pretrained=True).blocks.children())[3:7]\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(320, 1280, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.light_dense = nn.Sequential(nn.Linear(1, 32), nn.ReLU(inplace=True))\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(1280 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(1280 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_img, x_light):\n",
    "        x = self.initial_conv(x_img)\n",
    "        x = self.mnv2_block1(x)\n",
    "        x = self.channel_adapter(x)\n",
    "        x = self.mobilevit_encoder(x)\n",
    "        x = self.mvit_to_mnv2(x)\n",
    "        x = self.mnv2_block2(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.pool(x)\n",
    "        x_img_feat = torch.flatten(x, 1)\n",
    "        x_light_feat = self.light_dense(x_light)\n",
    "        x_concat = torch.cat([x_img_feat, x_light_feat], dim=1)\n",
    "        reg_out = self.reg_head(x_concat)\n",
    "        cls_out = self.cls_head(x_concat)\n",
    "        return reg_out, cls_out\n",
    "\n",
    "dataset = SoilDualTaskDataset(df, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True if device.type == \"cuda\" else False, num_workers=0)\n",
    "\n",
    "num_classes = len(df[\"moisture_class\"].unique())\n",
    "model = SoilNetDualHead(num_classes=num_classes).to(device)\n",
    "\n",
    "checkpoint_path = r\"C:\\Users\\PC\\soilNet\\checkpoints_VicReg_tinyImagenet\\vicreg_model_final_mu_30.0.pth\"\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model_state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        model_state_dict = checkpoint\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in model_state_dict.items() \n",
    "                       if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    temp_model_dict = model_dict.copy()\n",
    "    temp_model_dict.update(pretrained_dict)\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(temp_model_dict, strict=False)\n",
    "    print(f\"Loaded pretrained weights from {checkpoint_path}\")\n",
    "    if missing_keys:\n",
    "        print(f\"Some keys missing in checkpoint and not loaded: {missing_keys}\")\n",
    "    if unexpected_keys:\n",
    "        print(f\"Some keys in checkpoint not in model and ignored: {unexpected_keys}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Checkpoint not found: {checkpoint_path}. Using random initialization.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading checkpoint: {e}. Using random initialization.\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "reg_criterion = nn.MSELoss()\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 60\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss, total_reg_loss, total_cls_loss = 0, 0, 0\n",
    "    for batch in tqdm(dataloader, leave=False):\n",
    "        x_img = batch[\"image\"].to(device)\n",
    "        x_light = batch[\"light\"].to(device)\n",
    "        y_reg = batch[\"regression_target\"].to(device)\n",
    "        y_cls = batch[\"class_target\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_reg, pred_cls = model(x_img, x_light)\n",
    "        loss_reg = reg_criterion(pred_reg, y_reg)\n",
    "        loss_cls = cls_criterion(pred_cls, y_cls)\n",
    "        loss = loss_reg + loss_cls\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_reg_loss += loss_reg.item()\n",
    "        total_cls_loss += loss_cls.item()\n",
    "    avg_total = total_loss / len(dataloader)\n",
    "    loss_history.append((avg_total, total_reg_loss / len(dataloader), total_cls_loss / len(dataloader)))\n",
    "    print(f\"Loss â†’ Total: {avg_total:.4f}, Regression: {total_reg_loss / len(dataloader):.4f}, Classification: {total_cls_loss / len(dataloader):.4f}\")\n",
    "    model.eval()\n",
    "    y_true_reg, y_pred_reg = [], []\n",
    "    y_true_cls, y_pred_cls = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_img = batch[\"image\"].to(device)\n",
    "            x_light = batch[\"light\"].to(device)\n",
    "            y_reg = batch[\"regression_target\"].to(device)\n",
    "            y_cls = batch[\"class_target\"].to(device)\n",
    "            pred_reg, pred_cls = model(x_img, x_light)\n",
    "            y_true_reg.extend(y_reg.cpu().numpy())\n",
    "            y_pred_reg.extend(pred_reg.cpu().numpy())\n",
    "            y_true_cls.extend(y_cls.cpu().numpy())\n",
    "            y_pred_cls.extend(pred_cls.argmax(dim=1).cpu().numpy())\n",
    "    y_true_reg = np.array(y_true_reg) * 100\n",
    "    y_pred_reg = np.array(y_pred_reg) * 100\n",
    "    metrics = {}\n",
    "    for i, label in enumerate([\"SM_0\", \"SM_20\"]):\n",
    "        metrics[label] = {\n",
    "            \"RMSE\": mean_squared_error(y_true_reg[:, i], y_pred_reg[:, i]) ** 0.5,\n",
    "            \"MAE\": mean_absolute_error(y_true_reg[:, i], y_pred_reg[:, i]),\n",
    "            \"ME\": np.mean(y_pred_reg[:, i] - y_true_reg[:, i]),\n",
    "        }\n",
    "    metrics[\"Classification\"] = {\n",
    "        \"Accuracy\": accuracy_score(y_true_cls, y_pred_cls),\n",
    "        \"F1-score\": f1_score(y_true_cls, y_pred_cls, average=\"weighted\")\n",
    "    }\n",
    "    for label in [\"SM_0\", \"SM_20\"]:\n",
    "        print(f\"{label} â†’ RMSE: {metrics[label]['RMSE']:.8f}, MAE: {metrics[label]['MAE']:.8f}, ME: {metrics[label]['ME']:.8f}\")\n",
    "    print(f\"Classification â†’ Accuracy: {metrics['Classification']['Accuracy']:.8f}, F1-score: {metrics['Classification']['F1-score']:.8f}\")\n",
    "\n",
    "torch.save(model.state_dict(), r\"C:\\Users\\PC\\soilNet\\checkpoints_VicReg_tinyImagenet\\vicreg_model_final_tinyImageNet_mu_30.0.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
