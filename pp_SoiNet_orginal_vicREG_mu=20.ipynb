{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed2af90-41fe-421f-9437-c88c92c6a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6074e36-5ec1-418c-a3ed-7418a23fccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Kiểm tra và đặt thiết bị\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91bd6067-3711-4638-95c3-5a4091e0e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(df):\n",
    "    new_df = df.copy()\n",
    "    deleted_count = 0\n",
    "    valid_indices = []\n",
    "    for idx, row in tqdm(new_df.iterrows(), total=len(new_df)):\n",
    "        path = row[\"path\"]\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img.verify()\n",
    "            valid_indices.append(idx)\n",
    "        except (UnidentifiedImageError, FileNotFoundError, OSError) as e:\n",
    "            logging.info(f\"Image error: {path} ({str(e)})\")\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "                    deleted_count += 1\n",
    "                    logging.info(f\"Deleted image: {path}\")\n",
    "            except (PermissionError, OSError) as e:\n",
    "                logging.error(f\"Cannot delete image {path}: {str(e)}\")\n",
    "    new_df = new_df.loc[valid_indices].reset_index(drop=True)\n",
    "    logging.info(f\"Deleted {deleted_count} labeled images. {len(new_df)} images remain.\")\n",
    "    return new_df\n",
    "\n",
    "def preprocess_unlabeled_images(image_paths):\n",
    "    new_image_paths = []\n",
    "    deleted_count = 0\n",
    "    for path in tqdm(image_paths):\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img.verify()\n",
    "            new_image_paths.append(path)\n",
    "        except (UnidentifiedImageError, FileNotFoundError, OSError) as e:\n",
    "            logging.info(f\"Image error: {path} ({str(e)})\")\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "                    deleted_count += 1\n",
    "                    logging.info(f\"Deleted image: {path}\")\n",
    "            except (PermissionError, OSError) as e:\n",
    "                logging.error(f\"Cannot delete image {path}: {str(e)}\")\n",
    "    logging.info(f\"Deleted {deleted_count} unlabeled images. {len(new_image_paths)} images remain.\")\n",
    "    return new_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190a3635-2aaa-4852-8846-996eb51d127f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2057/2057 [01:02<00:00, 32.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2052/2052 [01:02<00:00, 32.99it/s]\n"
     ]
    }
   ],
   "source": [
    "label_csv_path = r\"F:\\Soil_Labeled_Data\\labels.csv\"\n",
    "fallback_dir = r\"F:\\Soil_Labeled_Data\\augmented_fallback\"\n",
    "os.makedirs(fallback_dir, exist_ok=True)\n",
    "df = pd.read_csv(label_csv_path)\n",
    "df = preprocess_images(df)\n",
    "\n",
    "augment = transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "replaced_count = 0\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    path = row[\"path\"]\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img.verify()\n",
    "    except (UnidentifiedImageError, FileNotFoundError, OSError):\n",
    "        folder = os.path.dirname(path)\n",
    "        all_images = [f for f in os.listdir(folder) if f.lower().endswith((\".jpg\", \".png\"))]\n",
    "        good_images = [f for f in all_images if f != os.path.basename(path)]\n",
    "        if not good_images:\n",
    "            continue\n",
    "        candidate = random.choice(good_images)\n",
    "        candidate_path = os.path.join(folder, candidate)\n",
    "        try:\n",
    "            img = Image.open(candidate_path).convert(\"RGB\")\n",
    "            img_aug = augment(img)\n",
    "            new_filename = f\"aug_{os.path.basename(path)}\"\n",
    "            new_path = os.path.join(fallback_dir, new_filename)\n",
    "            img_aug.save(new_path)\n",
    "            df.at[idx, \"path\"] = new_path\n",
    "            replaced_count += 1\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba635eb-f1dc-4fbd-a0d8-0227a00e0004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 11995/11995 [01:19<00:00, 150.02it/s]\n"
     ]
    }
   ],
   "source": [
    "image_dir = r\"F:/unlabeled_images\"\n",
    "image_paths = []\n",
    "for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"]:\n",
    "    image_paths.extend(glob.glob(os.path.join(image_dir, \"**\", ext), recursive=True))\n",
    "image_paths = preprocess_unlabeled_images(image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96588c62-959b-4472-be0f-fbcc5eb1f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "labeled_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            img1 = self.transform(image)\n",
    "            img2 = self.transform(image)\n",
    "            return img1, img2\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.image_paths))\n",
    "\n",
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['path']\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            img = self.transform(image)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.df))\n",
    "        humidity = torch.tensor([row['SM_0'] / 100, row['SM_20'] / 100], dtype=torch.float32)\n",
    "        class_label = torch.tensor(row[\"moisture_class\"], dtype=torch.long)\n",
    "        return img, humidity, class_label\n",
    "\n",
    "unlabeled_dataset = UnlabeledImageDataset(image_paths, transform)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=True, drop_last=True, num_workers=0)\n",
    "labeled_dataset = LabeledImageDataset(df, labeled_transform)\n",
    "labeled_dataloader = DataLoader(labeled_dataset, batch_size=16, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "class SoilNetDualHead(nn.Module):\n",
    "    def __init__(self, num_classes=10, simclr_mode=False):\n",
    "        super().__init__()\n",
    "        self.simclr_mode = simclr_mode\n",
    "        self.initial_conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.mnv2_block1 = nn.Sequential(*list(\n",
    "            timm.create_model(\"mobilenetv2_100.ra_in1k\", pretrained=True).blocks.children())[0:3]\n",
    "        )\n",
    "        self.channel_adapter = nn.Conv2d(32, 16, kernel_size=1, bias=False)\n",
    "        self.mobilevit_full = timm.create_model(\"mobilevitv2_050\", pretrained=True)\n",
    "        self.mobilevit_encoder = self.mobilevit_full.stages\n",
    "        self.mvit_to_mnv2 = nn.Conv2d(256, 32, kernel_size=1, bias=False)\n",
    "        self.mnv2_block2 = nn.Sequential(*list(\n",
    "            timm.create_model(\"mobilenetv2_100.ra_in1k\", pretrained=True).blocks.children())[3:7]\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(320, 1280, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.light_dense = nn.Sequential(nn.Linear(1, 32), nn.ReLU(inplace=True))\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(1280 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(1280 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_img, x_light=None):\n",
    "        x = self.initial_conv(x_img)\n",
    "        x = self.mnv2_block1(x)\n",
    "        x = self.channel_adapter(x)\n",
    "        x = self.mobilevit_encoder(x)\n",
    "        x = self.mvit_to_mnv2(x)\n",
    "        x = self.mnv2_block2(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.pool(x)\n",
    "        x_img_feat = torch.flatten(x, 1)\n",
    "        if self.simclr_mode:\n",
    "            return x_img_feat\n",
    "        x_light_feat = self.light_dense(x_light)\n",
    "        x_concat = torch.cat([x_img_feat, x_light_feat], dim=1)\n",
    "        reg_out = self.reg_head(x_concat)\n",
    "        cls_out = self.cls_head(x_concat)\n",
    "        return reg_out, cls_out\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, input_dim=1280, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_dim, proj_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class OnlineLinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim=1280, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class OnlineClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1280, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def vicreg_loss(z1, z2, lambda_=25.0, mu=25.0, nu=1.0, epsilon=1e-4):\n",
    "    invariance_loss = F.mse_loss(z1, z2)\n",
    "    def variance_term(z):\n",
    "        z_std = torch.sqrt(z.var(dim=0) + epsilon)\n",
    "        return torch.mean(F.relu(1 - z_std))\n",
    "    var_loss = variance_term(z1) + variance_term(z2)\n",
    "    def covariance_term(z):\n",
    "        z = z - z.mean(dim=0)\n",
    "        cov = (z.T @ z) / (z.shape[0] - 1)\n",
    "        off_diag = cov - torch.diag(cov.diag())\n",
    "        return off_diag.pow(2).sum() / z.shape[1]\n",
    "    cov_loss = covariance_term(z1) + covariance_term(z2)\n",
    "    return lambda_ * invariance_loss + mu * var_loss + nu * cov_loss\n",
    "\n",
    "model = SoilNetDualHead(num_classes=10, simclr_mode=True).to(device)\n",
    "projector = Projector(input_dim=1280, proj_dim=128).to(device)\n",
    "linear_reg = OnlineLinearRegression(input_dim=1280, output_dim=2).to(device)\n",
    "classifier = OnlineClassifier(input_dim=1280, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89271f6e-e617-4fb5-84bf-ee8d8d7132f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(r\"C:\\Users\\PC\\soilNet\\Model\\SoilNet_orginal.pth\", map_location=device))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "optimizer_vicreg = torch.optim.Adam(list(model.parameters()) + list(projector.parameters()), lr=1e-4)\n",
    "optimizer_linear = torch.optim.Adam(linear_reg.parameters(), lr=1e-3)\n",
    "optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "#checkpoint_dir = \"/content/drive/MyDrive/SoilNet_Checkpoints/checkpoints_VicReg\"\n",
    "checkpoint_dir = r\"C:\\Users\\PC\\soilNet\\checkpoints_VicReg\"\n",
    "\n",
    "#C:\\Users\\PC\\soilNet\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e040f89a-3f5e-47ed-ad72-c2edf6eff441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   1/120 (mu=20.0) - VICReg Loss: 30.2918, MSE: 1707.3518, RMSE: 41.3201, MAE: 31.6941, Accuracy: 0.16371829, F1-Score: 0.16588059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   2/120 (mu=20.0) - VICReg Loss: 29.0753, MSE: 1107.5518, RMSE: 33.2799, MAE: 26.5737, Accuracy: 0.17356475, F1-Score: 0.17631756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   3/120 (mu=20.0) - VICReg Loss: 27.9533, MSE: 943.5614, RMSE: 30.7174, MAE: 24.5197, Accuracy: 0.18649866, F1-Score: 0.18869783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   4/120 (mu=20.0) - VICReg Loss: 27.0730, MSE: 869.9637, RMSE: 29.4951, MAE: 23.5540, Accuracy: 0.19709613, F1-Score: 0.20079906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   5/120 (mu=20.0) - VICReg Loss: 26.3747, MSE: 890.9839, RMSE: 29.8494, MAE: 23.9792, Accuracy: 0.19475968, F1-Score: 0.20004091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   6/120 (mu=20.0) - VICReg Loss: 25.7695, MSE: 866.7008, RMSE: 29.4398, MAE: 23.5084, Accuracy: 0.19859813, F1-Score: 0.20143564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   7/120 (mu=20.0) - VICReg Loss: 25.2438, MSE: 859.1051, RMSE: 29.3105, MAE: 23.4239, Accuracy: 0.20210280, F1-Score: 0.20739327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   8/120 (mu=20.0) - VICReg Loss: 24.5000, MSE: 822.2408, RMSE: 28.6747, MAE: 22.9192, Accuracy: 0.20886182, F1-Score: 0.21323232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch   9/120 (mu=20.0) - VICReg Loss: 23.7668, MSE: 825.0011, RMSE: 28.7228, MAE: 23.0037, Accuracy: 0.20727637, F1-Score: 0.21142725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  10/120 (mu=20.0) - VICReg Loss: 22.9998, MSE: 846.8200, RMSE: 29.1002, MAE: 23.0956, Accuracy: 0.22012684, F1-Score: 0.22421257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  11/120 (mu=20.0) - VICReg Loss: 22.3032, MSE: 802.1143, RMSE: 28.3216, MAE: 22.6283, Accuracy: 0.21845794, F1-Score: 0.22063121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  12/120 (mu=20.0) - VICReg Loss: 21.8517, MSE: 791.1257, RMSE: 28.1270, MAE: 22.4528, Accuracy: 0.21920895, F1-Score: 0.22536550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  13/120 (mu=20.0) - VICReg Loss: 21.1206, MSE: 861.3548, RMSE: 29.3488, MAE: 23.4442, Accuracy: 0.22338117, F1-Score: 0.22500028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  14/120 (mu=20.0) - VICReg Loss: 20.5143, MSE: 824.3262, RMSE: 28.7111, MAE: 23.0057, Accuracy: 0.22755340, F1-Score: 0.23114863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  15/120 (mu=20.0) - VICReg Loss: 19.9607, MSE: 848.1754, RMSE: 29.1235, MAE: 23.3628, Accuracy: 0.23064085, F1-Score: 0.23499131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  16/120 (mu=20.0) - VICReg Loss: 19.4996, MSE: 842.5318, RMSE: 29.0264, MAE: 23.3159, Accuracy: 0.22897196, F1-Score: 0.23302683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  17/120 (mu=20.0) - VICReg Loss: 18.9157, MSE: 814.1770, RMSE: 28.5338, MAE: 22.9361, Accuracy: 0.22680240, F1-Score: 0.23217014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  18/120 (mu=20.0) - VICReg Loss: 18.6305, MSE: 845.6492, RMSE: 29.0800, MAE: 23.3981, Accuracy: 0.22329773, F1-Score: 0.22622167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  19/120 (mu=20.0) - VICReg Loss: 18.2930, MSE: 814.6902, RMSE: 28.5428, MAE: 22.8432, Accuracy: 0.23765020, F1-Score: 0.24082896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  20/120 (mu=20.0) - VICReg Loss: 18.0655, MSE: 786.4304, RMSE: 28.0434, MAE: 22.4957, Accuracy: 0.24057076, F1-Score: 0.24374722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  21/120 (mu=20.0) - VICReg Loss: 17.7634, MSE: 809.8091, RMSE: 28.4571, MAE: 22.7309, Accuracy: 0.23915220, F1-Score: 0.24348927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  22/120 (mu=20.0) - VICReg Loss: 17.5669, MSE: 848.2107, RMSE: 29.1241, MAE: 23.2644, Accuracy: 0.24132176, F1-Score: 0.24435338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  23/120 (mu=20.0) - VICReg Loss: 17.3886, MSE: 797.5852, RMSE: 28.2416, MAE: 22.6776, Accuracy: 0.24966622, F1-Score: 0.25391984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  24/120 (mu=20.0) - VICReg Loss: 17.1967, MSE: 785.6401, RMSE: 28.0293, MAE: 22.4869, Accuracy: 0.24724633, F1-Score: 0.25078603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  25/120 (mu=20.0) - VICReg Loss: 17.0677, MSE: 809.6745, RMSE: 28.4548, MAE: 22.7324, Accuracy: 0.25383845, F1-Score: 0.25775333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  26/120 (mu=20.0) - VICReg Loss: 16.9728, MSE: 822.0001, RMSE: 28.6705, MAE: 22.8991, Accuracy: 0.24307410, F1-Score: 0.24390795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  27/120 (mu=20.0) - VICReg Loss: 16.8708, MSE: 796.7246, RMSE: 28.2263, MAE: 22.6353, Accuracy: 0.24749666, F1-Score: 0.25092522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  28/120 (mu=20.0) - VICReg Loss: 16.7559, MSE: 806.8710, RMSE: 28.4055, MAE: 22.8305, Accuracy: 0.24457610, F1-Score: 0.24676546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  29/120 (mu=20.0) - VICReg Loss: 16.6074, MSE: 862.2025, RMSE: 29.3633, MAE: 23.4024, Accuracy: 0.24299065, F1-Score: 0.24826296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  30/120 (mu=20.0) - VICReg Loss: 16.5428, MSE: 863.6769, RMSE: 29.3884, MAE: 23.6501, Accuracy: 0.24532710, F1-Score: 0.24829674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  31/120 (mu=20.0) - VICReg Loss: 16.4729, MSE: 838.8663, RMSE: 28.9632, MAE: 23.3396, Accuracy: 0.24883178, F1-Score: 0.25250632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  32/120 (mu=20.0) - VICReg Loss: 16.4506, MSE: 828.2980, RMSE: 28.7802, MAE: 23.0958, Accuracy: 0.24516021, F1-Score: 0.24984761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  33/120 (mu=20.0) - VICReg Loss: 16.3822, MSE: 883.5209, RMSE: 29.7241, MAE: 24.0062, Accuracy: 0.24449266, F1-Score: 0.24850575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  34/120 (mu=20.0) - VICReg Loss: 16.3355, MSE: 843.9885, RMSE: 29.0515, MAE: 23.3699, Accuracy: 0.24616155, F1-Score: 0.24791245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  35/120 (mu=20.0) - VICReg Loss: 16.2381, MSE: 867.2885, RMSE: 29.4498, MAE: 23.5976, Accuracy: 0.24824766, F1-Score: 0.25246413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  36/120 (mu=20.0) - VICReg Loss: 16.1894, MSE: 866.1374, RMSE: 29.4302, MAE: 23.7181, Accuracy: 0.24123832, F1-Score: 0.24581962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  37/120 (mu=20.0) - VICReg Loss: 16.1961, MSE: 888.9770, RMSE: 29.8157, MAE: 24.1629, Accuracy: 0.24265688, F1-Score: 0.24389269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  38/120 (mu=20.0) - VICReg Loss: 16.0754, MSE: 889.4614, RMSE: 29.8238, MAE: 23.9503, Accuracy: 0.23956943, F1-Score: 0.24240128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  39/120 (mu=20.0) - VICReg Loss: 16.0606, MSE: 898.6681, RMSE: 29.9778, MAE: 24.1535, Accuracy: 0.23531375, F1-Score: 0.23870034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  40/120 (mu=20.0) - VICReg Loss: 16.0012, MSE: 886.3901, RMSE: 29.7723, MAE: 23.9591, Accuracy: 0.23381175, F1-Score: 0.23539691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  41/120 (mu=20.0) - VICReg Loss: 15.9843, MSE: 907.4931, RMSE: 30.1246, MAE: 24.3277, Accuracy: 0.24240654, F1-Score: 0.24635870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  42/120 (mu=20.0) - VICReg Loss: 15.9297, MSE: 918.3317, RMSE: 30.3040, MAE: 24.4209, Accuracy: 0.23781709, F1-Score: 0.23915803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  43/120 (mu=20.0) - VICReg Loss: 15.9501, MSE: 876.4287, RMSE: 29.6045, MAE: 23.8471, Accuracy: 0.24482644, F1-Score: 0.24739962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  44/120 (mu=20.0) - VICReg Loss: 15.8115, MSE: 894.7246, RMSE: 29.9119, MAE: 24.0984, Accuracy: 0.24399199, F1-Score: 0.24588042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  45/120 (mu=20.0) - VICReg Loss: 15.8408, MSE: 886.5277, RMSE: 29.7746, MAE: 23.9218, Accuracy: 0.23639853, F1-Score: 0.23689831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  46/120 (mu=20.0) - VICReg Loss: 15.8061, MSE: 882.8602, RMSE: 29.7130, MAE: 23.9984, Accuracy: 0.23564753, F1-Score: 0.23807933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  47/120 (mu=20.0) - VICReg Loss: 15.8462, MSE: 926.1725, RMSE: 30.4331, MAE: 24.6063, Accuracy: 0.23531375, F1-Score: 0.23507786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  48/120 (mu=20.0) - VICReg Loss: 15.7317, MSE: 918.4377, RMSE: 30.3057, MAE: 24.3217, Accuracy: 0.23205941, F1-Score: 0.23219460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  49/120 (mu=20.0) - VICReg Loss: 15.8138, MSE: 931.3066, RMSE: 30.5173, MAE: 24.5430, Accuracy: 0.23272697, F1-Score: 0.23632054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  50/120 (mu=20.0) - VICReg Loss: 15.7247, MSE: 921.7116, RMSE: 30.3597, MAE: 24.4907, Accuracy: 0.23739987, F1-Score: 0.24081454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  51/120 (mu=20.0) - VICReg Loss: 15.8014, MSE: 935.3412, RMSE: 30.5833, MAE: 24.6495, Accuracy: 0.24032043, F1-Score: 0.23966586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  52/120 (mu=20.0) - VICReg Loss: 15.6978, MSE: 915.5172, RMSE: 30.2575, MAE: 24.2626, Accuracy: 0.24148865, F1-Score: 0.24383748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  53/120 (mu=20.0) - VICReg Loss: 15.7209, MSE: 892.7245, RMSE: 29.8785, MAE: 23.9620, Accuracy: 0.24457610, F1-Score: 0.24378070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  54/120 (mu=20.0) - VICReg Loss: 15.6453, MSE: 909.0009, RMSE: 30.1496, MAE: 24.2292, Accuracy: 0.23998665, F1-Score: 0.24179147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  55/120 (mu=20.0) - VICReg Loss: 15.6421, MSE: 954.1391, RMSE: 30.8891, MAE: 24.9824, Accuracy: 0.23714953, F1-Score: 0.23886068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  56/120 (mu=20.0) - VICReg Loss: 15.6638, MSE: 908.1471, RMSE: 30.1355, MAE: 24.2312, Accuracy: 0.24274032, F1-Score: 0.24353826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  57/120 (mu=20.0) - VICReg Loss: 15.5769, MSE: 916.2975, RMSE: 30.2704, MAE: 24.3864, Accuracy: 0.24115487, F1-Score: 0.24161702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  58/120 (mu=20.0) - VICReg Loss: 15.5731, MSE: 897.4815, RMSE: 29.9580, MAE: 24.2105, Accuracy: 0.23865154, F1-Score: 0.23967605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  59/120 (mu=20.0) - VICReg Loss: 15.6471, MSE: 921.0000, RMSE: 30.3480, MAE: 24.4261, Accuracy: 0.24173899, F1-Score: 0.24158335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  60/120 (mu=20.0) - VICReg Loss: 15.5348, MSE: 927.6979, RMSE: 30.4581, MAE: 24.5979, Accuracy: 0.23514686, F1-Score: 0.23600903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  61/120 (mu=20.0) - VICReg Loss: 15.5861, MSE: 946.3391, RMSE: 30.7626, MAE: 24.7894, Accuracy: 0.23656542, F1-Score: 0.23567303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  62/120 (mu=20.0) - VICReg Loss: 15.5429, MSE: 943.1864, RMSE: 30.7113, MAE: 24.6857, Accuracy: 0.23865154, F1-Score: 0.23939850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch  63/120 (mu=20.0) - VICReg Loss: 15.5649, MSE: 929.6638, RMSE: 30.4904, MAE: 24.5138, Accuracy: 0.23406208, F1-Score: 0.23473586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m#for mu_val in [20.0, 23.0, 25.0, 27.0, 30.0, 33.0]:\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mu_val \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m20.0\u001b[39m]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[43mtrain_vicreg_with_mu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Convert metrics to DataFrame and save to CSV\u001b[39;00m\n\u001b[0;32m    124\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_metrics)\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mtrain_vicreg_with_mu\u001b[1;34m(mu, num_epochs, metrics_df)\u001b[0m\n\u001b[0;32m     18\u001b[0m running_vicreg_loss \u001b[38;5;241m=\u001b[39m running_mse \u001b[38;5;241m=\u001b[39m running_mae \u001b[38;5;241m=\u001b[39m running_accuracy \u001b[38;5;241m=\u001b[39m running_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     19\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img1, img2 \u001b[38;5;129;01min\u001b[39;00m tqdm(unlabeled_dataloader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     img1, img2 \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mto(device), img2\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m     feat1 \u001b[38;5;241m=\u001b[39m model(img1, x_light\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m, in \u001b[0;36mUnlabeledImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[0;32m     27\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m img2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img1, img2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:540\u001b[0m, in \u001b[0;36mRandomApply.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 540\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1280\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1278\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1280\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torchvision\\transforms\\functional.py:968\u001b[0m, in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m    966\u001b[0m     _log_api_usage_once(adjust_hue)\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:117\u001b[0m, in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m    113\u001b[0m np_h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mint32(hue_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    115\u001b[0m h \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np_h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHSV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\train_env\\lib\\site-packages\\PIL\\Image.py:1146\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     dither \u001b[38;5;241m=\u001b[39m Dither\u001b[38;5;241m.\u001b[39mFLOYDSTEINBERG\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1146\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def train_vicreg_with_mu(mu=25.0, num_epochs=150, metrics_df=None):\n",
    "    vicreg_losses, mse_losses, rmse_losses, mae_losses, accuracy_scores, f1_scores = [], [], [], [], [], []\n",
    "    labeled_iterator = iter(labeled_dataloader)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        projector.train()\n",
    "        linear_reg.train()\n",
    "        classifier.train()\n",
    "        running_vicreg_loss = running_mse = running_mae = running_accuracy = running_f1 = 0.0\n",
    "        num_batches = 0\n",
    "        for img1, img2 in tqdm(unlabeled_dataloader, leave=False):\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            feat1 = model(img1, x_light=None)\n",
    "            feat2 = model(img2, x_light=None)\n",
    "            z1 = projector(feat1)\n",
    "            z2 = projector(feat2)\n",
    "            vicreg_loss_val = vicreg_loss(z1, z2, mu=mu)\n",
    "            optimizer_vicreg.zero_grad()\n",
    "            vicreg_loss_val.backward()\n",
    "            optimizer_vicreg.step()\n",
    "            try:\n",
    "                labeled_img, humidity, class_label = next(labeled_iterator)\n",
    "            except StopIteration:\n",
    "                labeled_iterator = iter(labeled_dataloader)\n",
    "                labeled_img, humidity, class_label = next(labeled_iterator)\n",
    "            labeled_img, humidity, class_label = labeled_img.to(device), humidity.to(device), class_label.to(device)\n",
    "            with torch.no_grad():\n",
    "                feat = model(labeled_img, x_light=None)\n",
    "            pred_humidity = linear_reg(feat)\n",
    "            mse_loss = F.mse_loss(pred_humidity, humidity)\n",
    "            optimizer_linear.zero_grad()\n",
    "            mse_loss.backward()\n",
    "            optimizer_linear.step()\n",
    "            pred_logits = classifier(feat)\n",
    "            cls_loss = F.cross_entropy(pred_logits, class_label)\n",
    "            optimizer_classifier.zero_grad()\n",
    "            cls_loss.backward()\n",
    "            optimizer_classifier.step()\n",
    "            pred_humidity_np = pred_humidity.detach().cpu().numpy() * 100\n",
    "            humidity_np = humidity.detach().cpu().numpy() * 100\n",
    "            mse = mean_squared_error(humidity_np, pred_humidity_np)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(humidity_np, pred_humidity_np)\n",
    "            pred_classes = torch.argmax(pred_logits, dim=1).detach().cpu().numpy()\n",
    "            true_classes = class_label.detach().cpu().numpy()\n",
    "            accuracy = accuracy_score(true_classes, pred_classes)\n",
    "            f1 = f1_score(true_classes, pred_classes, average='weighted')\n",
    "            running_vicreg_loss += vicreg_loss_val.item()\n",
    "            running_mse += mse\n",
    "            running_mae += mae\n",
    "            running_accuracy += accuracy\n",
    "            running_f1 += f1\n",
    "            num_batches += 1\n",
    "        avg_vicreg_loss = running_vicreg_loss / num_batches\n",
    "        avg_mse = running_mse / num_batches\n",
    "        avg_rmse = np.sqrt(avg_mse)\n",
    "        avg_mae = running_mae / num_batches\n",
    "        avg_accuracy = running_accuracy / num_batches\n",
    "        avg_f1 = running_f1 / num_batches\n",
    "        vicreg_losses.append(avg_vicreg_loss)\n",
    "        mse_losses.append(avg_mse)\n",
    "        rmse_losses.append(avg_rmse)\n",
    "        mae_losses.append(avg_mae)\n",
    "        accuracy_scores.append(avg_accuracy)\n",
    "        f1_scores.append(avg_f1)\n",
    "        \n",
    "        print(f\"✅ Epoch {epoch:3d}/{num_epochs} (mu={mu}) - VICReg Loss: {avg_vicreg_loss:.4f}, \"\n",
    "              f\"MSE: {avg_mse:.4f}, RMSE: {avg_rmse:.4f}, MAE: {avg_mae:.4f}, \"\n",
    "              f\"Accuracy: {avg_accuracy:.8f}, F1-Score: {avg_f1:.8f}\")\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'projector_state_dict': projector.state_dict(),\n",
    "            'linear_reg_state_dict': linear_reg.state_dict(),\n",
    "            'classifier_state_dict': classifier.state_dict(),\n",
    "            'vicreg_loss': avg_vicreg_loss,\n",
    "            'mse_loss': avg_mse,\n",
    "            'rmse_loss': avg_rmse,\n",
    "            'mae_loss': avg_mae,\n",
    "            'accuracy': avg_accuracy,\n",
    "            'f1_score': avg_f1\n",
    "        }\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'vicreg_mu_{mu}_epoch_{epoch}.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logging.info(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "        final_model_path = os.path.join(checkpoint_dir, f'vicreg_model_final_mu_{mu}.pth')\n",
    "        final_projector_path = os.path.join(checkpoint_dir, f'vicreg_projector_final_mu_{mu}.pth')\n",
    "        final_linear_reg_path = os.path.join(checkpoint_dir, f'vicreg_linear_reg_final_mu_{mu}.pth')\n",
    "        final_classifier_path = os.path.join(checkpoint_dir, f'vicreg_classifier_final_mu_{mu}.pth')\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        torch.save(projector.state_dict(), final_projector_path)\n",
    "        torch.save(linear_reg.state_dict(), final_linear_reg_path)\n",
    "        torch.save(classifier.state_dict(), final_classifier_path)\n",
    "        logging.info(f\"Saved final models (mu={mu}): {final_model_path}, {final_projector_path}, {final_linear_reg_path}, {final_classifier_path}\")\n",
    "        # Append metrics to DataFrame\n",
    "        metrics_df.append({\n",
    "            'mu': mu,\n",
    "            'epoch': epoch,\n",
    "            'vicreg_loss': avg_vicreg_loss,\n",
    "            'mse_loss': avg_mse,\n",
    "            'rmse_loss': avg_rmse,\n",
    "            'mae_loss': avg_mae,\n",
    "            'accuracy': avg_accuracy,\n",
    "            'f1_score': avg_f1\n",
    "        })\n",
    "\n",
    "# Initialize an empty list to collect metrics\n",
    "all_metrics = []\n",
    "#for mu_val in [20.0, 23.0, 25.0, 27.0, 30.0, 33.0]:\n",
    "for mu_val in [20.0]:\n",
    "    train_vicreg_with_mu(mu=mu_val, num_epochs=120, metrics_df=all_metrics)\n",
    "\n",
    "# Convert metrics to DataFrame and save to CSV\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "metrics_df.to_csv(os.path.join(checkpoint_dir, 'vicreg_metrics.csv'), index=False)\n",
    "logging.info(f\"Saved metrics to {os.path.join(checkpoint_dir, 'import torch')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
